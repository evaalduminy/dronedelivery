"""
Hybrid Neuro-Symbolic Controller
Combines Q-Learning (Neural) with Logic Engine (Symbolic)
"""

from typing import Dict, List, Tuple, Optional
import numpy as np

from .q_learning import QLearningAgent
from .logic_engine import LogicEngine, RuleType
from ..utils.config import ACTIONS
from ..utils.logger import get_logger


class HybridController:
    """
    Ø§Ù„Ù…ØªØ­ÙƒÙ… Ø§Ù„Ù‡Ø¬ÙŠÙ† (Neuro-Symbolic)
    
    ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ†:
    - Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¹ØµØ¨ÙŠØ© (Q-Learning): Ù„Ù„ØªØ¹Ù„Ù… ÙˆØ§Ù„ÙƒÙØ§Ø¡Ø©
    - Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø±Ù…Ø²ÙŠØ© (Logic Engine): Ù„Ù„Ø£Ù…Ø§Ù† ÙˆØ§Ù„Ù‚ÙˆØ§Ø¹Ø¯
    
    Ø§Ù„Ù…Ø¨Ø¯Ø£:
    1. Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ ÙŠØ­Ø¯Ø¯ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø¢Ù…Ù†Ø©
    2. Q-Learning ÙŠØ®ØªØ§Ø± Ø§Ù„Ø£ÙØ¶Ù„ Ù…Ù† Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø¢Ù…Ù†Ø©
    3. Ø¶Ù…Ø§Ù† Ø§Ù„Ø£Ù…Ø§Ù† Ù…Ø¹ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡
    """
    
    def __init__(self, actions: List[str] = ACTIONS):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ØªØ­ÙƒÙ… Ø§Ù„Ù‡Ø¬ÙŠÙ†
        
        Args:
            actions: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ù…ÙƒÙ†Ø©
        """
        self.actions = actions
        
        # Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        self.q_agent = QLearningAgent(actions)
        self.logic_engine = LogicEngine()
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self.decisions_made = 0
        self.safety_overrides = 0  # Ø¹Ø¯Ø¯ Ù…Ø±Ø§Øª ØªØ¯Ø®Ù„ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø£Ù…Ø§Ù†
        self.logic_suggestions = 0  # Ø¹Ø¯Ø¯ Ù…Ø±Ø§Øª Ø§Ù‚ØªØ±Ø§Ø­ Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ
        
        self.logger = get_logger()
        self.logger.info("Hybrid Controller initialized")
    
    def choose_action(self, state: Dict, training: bool = True) -> Tuple[str, Dict]:
        """
        Ø§Ø®ØªÙŠØ§Ø± Ø¥Ø¬Ø±Ø§Ø¡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù‡Ø¬ Ø§Ù„Ù‡Ø¬ÙŠÙ†
        
        Args:
            state: Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©
            training: Ù‡Ù„ Ù†Ø­Ù† ÙÙŠ ÙˆØ¶Ø¹ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŸ
        
        Returns:
            tuple Ù…Ù† (Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„Ù…Ø®ØªØ§Ø±ØŒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù‚Ø±Ø§Ø±)
        """
        self.decisions_made += 1
        
        # 1. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø§Ù„Ø© Ø¨Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ
        triggered_rules = self.logic_engine.get_triggered_rules(state)
        recommended_action, top_rule = self.logic_engine.get_recommended_action(state)
        
        # 2. Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø¢Ù…Ù†Ø©
        safe_actions = self.logic_engine.get_valid_actions(state, self.actions)
        
        # 3. Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù‚Ø±Ø§Ø±
        decision_info = {
            'triggered_rules': len(triggered_rules),
            'top_rule': top_rule.name if top_rule else None,
            'safe_actions_count': len(safe_actions),
            'recommended_action': recommended_action,
            'decision_type': None,
            'q_values': {},
            'safety_override': False
        }
        
        # 4. Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©
        
        # Ø£) Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø£Ù…Ø§Ù† Ø§Ù„Ø­Ø±Ø¬Ø© (Ø£ÙˆÙ„ÙˆÙŠØ© Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹)
        critical_rules = [r for r in triggered_rules 
                         if r.rule_type == RuleType.SAFETY and r.priority >= 90]
        
        if critical_rules:
            # ØªØ¯Ø®Ù„ ÙÙˆØ±ÙŠ Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø£Ù…Ø§Ù† Ø§Ù„Ø­Ø±Ø¬Ø©
            action = critical_rules[0].action
            decision_info['decision_type'] = 'safety_critical'
            decision_info['safety_override'] = True
            self.safety_overrides += 1
            
            self.logger.warning(f"Safety override: {critical_rules[0].name} -> {action}")
        
        # Ø¨) Ù‚ÙˆØ§Ø¹Ø¯ Ù…Ù†Ø·Ù‚ÙŠØ© Ù…Ø¹ Ø®ÙŠØ§Ø±Ø§Øª Q-Learning Ø£Ùˆ Ø§Ù„ØªÙˆØ¬Ù‡ Ù„Ù„Ù‡Ø¯Ù
        elif safe_actions:
            # ğŸ¯ Ù…ÙŠØ²Ø© Ø§Ù„ØªÙˆØ¬Ù‡ Ù„Ù„Ù‡Ø¯Ù (Goal-Oriented)
            # Ù†Ø·Ø¨Ù‚ Ø§Ù„Ù‡ÙŠÙˆØ±Ø³ØªÙŠÙƒ Ø¥Ø°Ø§ ÙƒÙ†Ø§ ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© (Exploration) Ø£Ùˆ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù„Ø¯Ù‰ Ø§Ù„ÙˆÙƒÙŠÙ„ Ø®Ø¨Ø±Ø© ÙƒØ§ÙÙŠØ©
            best_q_action = self.q_agent.get_best_action_greedy(state, safe_actions)
            q_val = self.q_agent.get_q_value(state, best_q_action)
            
            # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø§Ù‹ (Q near 0) Ø£Ùˆ ÙƒÙ†Ø§ ÙÙŠ ÙˆØ¶Ø¹ Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§ÙØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªÙˆØ¬Ù‡ Ù„Ù„Ù‡Ø¯Ù
            if (training and self.q_agent.epsilon > 0.3) or (q_val < 0.1):
                best_move = self._get_goal_oriented_action(state, safe_actions)
                if best_move:
                    action = best_move
                    decision_info['decision_type'] = 'goal_oriented_heuristic'
                else:
                    action = best_q_action
                    decision_info['decision_type'] = 'hybrid_greedy'
            else:
                action = best_q_action
                decision_info['decision_type'] = 'hybrid_greedy'
                
            # Ø­Ø³Ø§Ø¨ Q-values Ù„Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø¢Ù…Ù†Ø©
            for safe_action in safe_actions:
                decision_info['q_values'][safe_action] = self.q_agent.get_q_value(state, safe_action)
        
        # Ø¬) Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø¢Ù…Ù†Ø© - Ø¥Ø¬Ø±Ø§Ø¡ Ø·ÙˆØ§Ø±Ø¦
        else:
            action = "emergency_land"
            decision_info['decision_type'] = 'emergency'
            decision_info['safety_override'] = True
            self.safety_overrides += 1
            
            self.logger.error("No safe actions available - emergency landing")
        
        # 5. ØªØ­Ù‚Ù‚ Ø¥Ø¶Ø§ÙÙŠ Ù…Ù† Ø§Ù„Ø£Ù…Ø§Ù†
        is_safe, violated_rules = self.logic_engine.is_action_safe(state, action)
        if not is_safe and decision_info['decision_type'] != 'emergency':
            # Ø¥Ø¬Ø±Ø§Ø¡ ØºÙŠØ± Ø¢Ù…Ù† - ØªØºÙŠÙŠØ± Ù„Ù„Ø§Ù†ØªØ¸Ø§Ø±
            action = "wait"
            decision_info['safety_override'] = True
            decision_info['violated_rules'] = [r.name for r in violated_rules]
            self.safety_overrides += 1
            
            self.logger.warning(f"Action changed to 'wait' due to safety violations")
        
        # 6. ØªØ³Ø¬ÙŠÙ„ Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ
        if recommended_action and action == recommended_action:
            self.logic_suggestions += 1
            
        return action, decision_info

    def _get_goal_oriented_action(self, state: Dict, safe_actions: List[str]) -> Optional[str]:
        """Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„Ø°ÙŠ ÙŠÙ‚Ø±Ø¨ Ø§Ù„Ø¯Ø±ÙˆÙ† Ù…Ù† Ø§Ù„Ù‡Ø¯Ù Ù…Ù† Ø¨ÙŠÙ† Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø¢Ù…Ù†Ø©"""
        if 'relative_target' not in state:
            return None
            
        dx, dy, dz = state['relative_target']
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø­Ø³Ø¨ Ù…Ø¯Ù‰ ØªÙ‚Ù„ÙŠÙ„Ù‡Ø§ Ù„Ù„Ù…Ø³Ø§ÙØ©
        best_action = None
        min_dist = float('inf')
        
        for action in safe_actions:
            adx, ady, adz = 0, 0, 0
            if action == 'MOVE_NORTH': ady = -1
            elif action == 'MOVE_SOUTH': ady = 1
            elif action == 'MOVE_EAST': adx = 1
            elif action == 'MOVE_WEST': adx = -1
            elif action == 'MOVE_UP': adz = 1
            elif action == 'MOVE_DOWN': adz = -1
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³Ø§ÙØ© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© (Manhattan distance)
            new_dist = abs(dx - adx) + abs(dy - ady) + abs(dz - adz)
            if new_dist < min_dist:
                min_dist = new_dist
                best_action = action
                
        # Ù†Ø®ØªØ§Ø± Ø§Ù„Ø­Ø±ÙƒØ© ÙÙ‚Ø· Ø¥Ø°Ø§ ÙƒØ§Ù†Øª ÙØ¹Ù„Ø§Ù‹ ØªÙ‚Ø±Ø¨Ù†Ø§ Ù…Ù† Ø§Ù„Ù‡Ø¯Ù
        current_dist = abs(dx) + abs(dy) + abs(dz)
        if min_dist < current_dist:
            return best_action
            
        return None
    
    def update(self, state: Dict, action: str, reward: float, 
               next_state: Dict, done: bool):
        """
        ØªØ­Ø¯ÙŠØ« Q-Learning Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¬Ø±Ø¨Ø©
        
        Args:
            state: Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©
            action: Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„Ù…Ù†ÙØ°
            reward: Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©
            next_state: Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©
            done: Ù‡Ù„ Ø§Ù†ØªÙ‡Øª Ø§Ù„Ø­Ù„Ù‚Ø©ØŸ
        """
        # ØªØ­Ø¯ÙŠØ« Q-Learning ÙÙ‚Ø·
        self.q_agent.update(state, action, reward, next_state, done)
    
    def get_action_explanation(self, state: Dict, action: str, decision_info: Dict) -> str:
        """
        Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø´Ø±Ø­ Ù…ÙØµÙ„ Ù„Ù„Ù‚Ø±Ø§Ø±
        
        Args:
            state: Ø§Ù„Ø­Ø§Ù„Ø©
            action: Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„Ù…Ø®ØªØ§Ø±
            decision_info: Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù‚Ø±Ø§Ø±
        
        Returns:
            Ø´Ø±Ø­ Ù…ÙØµÙ„
        """
        explanation = f"Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„Ù…Ø®ØªØ§Ø±: {action}\n"
        explanation += f"Ù†ÙˆØ¹ Ø§Ù„Ù‚Ø±Ø§Ø±: {decision_info['decision_type']}\n"
        
        if decision_info['top_rule']:
            explanation += f"Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: {decision_info['top_rule']}\n"
        
        if decision_info['safety_override']:
            explanation += "âš ï¸ ØªØ¯Ø®Ù„ Ø£Ù…Ø§Ù† ÙÙˆØ±ÙŠ\n"
        
        explanation += f"Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø¢Ù…Ù†Ø©: {decision_info['safe_actions_count']}\n"
        explanation += f"Ø¹Ø¯Ø¯ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù…ÙØ¹Ù„Ø©: {decision_info['triggered_rules']}\n"
        
        if decision_info['q_values']:
            explanation += "\nQ-Values Ù„Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø¢Ù…Ù†Ø©:\n"
            for act, q_val in decision_info['q_values'].items():
                explanation += f"  {act}: {q_val:.3f}\n"
        
        return explanation
    
    def get_state_analysis(self, state: Dict) -> Dict:
        """
        ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„ Ù„Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©
        
        Args:
            state: Ø§Ù„Ø­Ø§Ù„Ø©
        
        Returns:
            ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„
        """
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ
        triggered_rules = self.logic_engine.get_triggered_rules(state)
        safe_actions = self.logic_engine.get_valid_actions(state, self.actions)
        recommended_action, top_rule = self.logic_engine.get_recommended_action(state)
        
        # ØªØ­Ù„ÙŠÙ„ Q-Learning
        q_values = {}
        for action in self.actions:
            q_values[action] = self.q_agent.get_q_value(state, action)
        
        best_q_action = max(q_values, key=q_values.get)
        
        return {
            'logic_analysis': {
                'triggered_rules': [(r.name, r.priority, r.description) for r in triggered_rules],
                'safe_actions': safe_actions,
                'recommended_action': recommended_action,
                'top_rule': top_rule.name if top_rule else None
            },
            'q_learning_analysis': {
                'q_values': q_values,
                'best_action': best_q_action,
                'epsilon': self.q_agent.epsilon,
                'q_table_size': len(self.q_agent.q_table)
            },
            'hybrid_decision': {
                'would_choose': self.choose_action(state, training=False)[0],
                'safety_constraints': len(safe_actions) < len(self.actions)
            }
        }
    
    def train_episode(self, env, max_steps: int = 1000) -> Dict:
        """
        ØªØ¯Ø±ÙŠØ¨ Ø­Ù„Ù‚Ø© ÙˆØ§Ø­Ø¯Ø©
        
        Args:
            env: Ø§Ù„Ø¨ÙŠØ¦Ø©
            max_steps: Ø£Ù‚ØµÙ‰ Ø¹Ø¯Ø¯ Ø®Ø·ÙˆØ§Øª
        
        Returns:
            Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø­Ù„Ù‚Ø©
        """
        state = env.reset()
        total_reward = 0
        steps = 0
        safety_overrides = 0
        
        episode_log = []
        
        for step in range(max_steps):
            # Ø§Ø®ØªÙŠØ§Ø± Ø¥Ø¬Ø±Ø§Ø¡
            action, decision_info = self.choose_action(state, training=True)
            
            # ØªÙ†ÙÙŠØ° Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡
            next_state, reward, done, info = env.step(action)
            
            # ØªØ­Ø¯ÙŠØ« Q-Learning
            self.update(state, action, reward, next_state, done)
            
            # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            total_reward += reward
            steps += 1
            if decision_info['safety_override']:
                safety_overrides += 1
            
            # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø®Ø·ÙˆØ©
            episode_log.append({
                'step': step,
                'action': action,
                'reward': reward,
                'decision_type': decision_info['decision_type'],
                'safety_override': decision_info['safety_override']
            })
            
            state = next_state
            
            if done:
                break
        
        # ØªÙ‚Ù„ÙŠÙ„ epsilon
        self.q_agent.decay_epsilon()
        self.q_agent.reset_for_episode()
        
        return {
            'total_reward': total_reward,
            'steps': steps,
            'safety_overrides': safety_overrides,
            'success': info.get('success', False),
            'episode_log': episode_log
        }
    
    def save_models(self, q_table_path: str = None):
        """
        Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        
        Args:
            q_table_path: Ù…Ø³Ø§Ø± Ø­ÙØ¸ Q-table
        """
        self.q_agent.save(q_table_path)
        self.logger.info("Models saved successfully")
    
    def load_models(self, q_table_path: str = None):
        """
        ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        
        Args:
            q_table_path: Ù…Ø³Ø§Ø± Q-table
        """
        success = self.q_agent.load(q_table_path)
        if success:
            self.logger.info("Models loaded successfully")
        return success
    
    def get_statistics(self) -> Dict:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø´Ø§Ù…Ù„Ø©"""
        q_stats = self.q_agent.get_statistics()
        logic_stats = self.logic_engine.get_statistics()
        
        return {
            'hybrid_controller': {
                'decisions_made': self.decisions_made,
                'safety_overrides': self.safety_overrides,
                'logic_suggestions': self.logic_suggestions,
                'safety_override_rate': self.safety_overrides / max(self.decisions_made, 1),
                'logic_suggestion_rate': self.logic_suggestions / max(self.decisions_made, 1)
            },
            'q_learning': q_stats,
            'logic_engine': logic_stats
        }
    
    def reset_statistics(self):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"""
        self.decisions_made = 0
        self.safety_overrides = 0
        self.logic_suggestions = 0
    
    def __repr__(self) -> str:
        return (f"HybridController(decisions={self.decisions_made}, "
                f"safety_overrides={self.safety_overrides})")